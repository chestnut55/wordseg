# wordseg
分词算法实验

语料：http://sighan.cs.uchicago.edu/bakeoff2005/data/icwb2-data.zip

最大概率分词：max_proba_tokenizer.py  
precious: 0.925911  
recall: 0.996810  
F1: 0.960053  

隐马尔可夫分词：hmm_tokenizer.py 
precious: 0.984371  
recall: 0.966150  
F1: 0.975176  

# keras-contrib
!pip install git+https://www.github.com/keras-team/keras-contrib.git

